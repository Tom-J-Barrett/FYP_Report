\tocless\subsection{Objective}
There are many on line resources that are geared towards helping deep learning novices.
One of these resources is a course on Udemy titled 'Complete Guide to Tensorflow for Deep Learning with Python' \parencite{udemy}.
This course has a section on Convolutional Neural Networks which has been followed and completed.
The CNN used in this section is very simple as it is an introductory CNN.
It consists of two convolutional layers, two max pooling layers and a fully connected layer.

\begin{table}[h]
\centering
\caption{Udemy Tutorial}
\label{my-label}
\begin{tabular}{|l|p{8cm}|}
\hline
\textbf{Network Architecture} & A simple architecture was used for prototyping purposes            \\ \hline
\textbf{Dataset}              & MNIST dataset \\ \hline
\textbf{APIs and Libraries}   & TensorFlow                                                         \\ \hline
\end{tabular}
\end{table}

\tocless\subsection{Script}
The following Figures \ref{lst:helperFunctions}, \ref{lst:layerCreation}, and \ref{lst:trainingModel1} convey key aspects of the script needed to create and train this model.
Figure \ref{lst:helperFunctions} defines helper functions taht are used to initialise weights and biases, create convolutional layers, create pooling layers, and create fully connected connected layers.
Figure \ref{lst:layerCreation} uses these helper functions to create the layers for this network.
Finally the code required to train the model is outlined in Figure \ref{lst:trainingModel1}.


\begin{figure}[h]
\caption{Helper Functions \parencite{udemy}}
\label{lst:helperFunctions}
\begin{lstlisting}[style=Python]
#INIT WEIGHTS
def init_weights(shape):
    init_random_dist = tf.truncated_normal(shape, stddev = 0.1)
    return tf.Variable(init_random_dist)

#INIT BIAS
def init_bias(shape):
    init_bias_vals = tf.constant(0.1, shape = shape)
    return tf.Variable(init_bias_vals)

#CONV2D
def conv2d(x, W):
    #x -> [batch, H, W, Channels]
    #W -> [filterH, filterW, ChannelsIn, ChannelsOut]
    return tf.nn.conv2d(x, W, strides = [1,1,1,1], padding = 'SAME')

#POOLING
def max_pool_2by2(x):
    #x -> [batch, H, W, Channels]
    return tf.nn.max_pool(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

#NORMAL (FULLY CONNECTED)
def normal_full_layer(input_layer, size):
    input_size = int(input_layer.get_shape()[1])
    W = init_weights([input_size, size])
    b = init_bias([size])
    return tf.matmul(input_layer, W) + b
\end{lstlisting}
\end{figure}

\begin{figure}[h]
\caption{Layer Creation \parencite{udemy}}
\label{lst:layerCreation}
\begin{lstlisting}[style=Python]
#32 features for every 5 x 5 kernel with 1(grayscale)
convo_1 = convolutional_layer(x_image, shape = [5,5,1,32])
convo_1_pooling = max_pool_2by2(convo_1)

convo_2 = convolutional_layer(convo_1_pooling, shape = [5,5,32,64])
convo_2_pooling = max_pool_2by2(convo_2)

convo_2_flat = tf.reshape(convo_2_pooling, [-1,7*7*64])
full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat, 1024))
\end{lstlisting}
\end{figure}

\begin{figure}[h]
\caption{Training the Model \parencite{udemy}}
\label{lst:trainingModel1}
\begin{lstlisting}[style=Python]
with tf.Session() as sess:
    sess.run(init)
    
    #for every steo, get a batch and run it through the model
    for i in range(steps):
        batch_x, batch_y = mnist.train.next_batch(32)
        batch_test = mnist.test.next_batch(32)
        sess.run(train, feed_dict = {x:batch_x, y_true:batch_y, hold_prob:0.5})
        if i%500 == 0:
            print("ACCURACY: ")
            match = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))
            acc = tf.reduce_mean(tf.cast(match, tf.float32))
            print(sess.run(acc, feed_dict = {x:batch_test, y_true:mnist.test.labels, hold_prob:1.0}))
            print('\n')     
    sess.run(acc, feed_dict = {x:mnist.test.images} )
\end{lstlisting}
\end{figure}

\tocless\subsection{Results}
The final Top-1 accuracy for this experiment was of 97.3\%, calculated from a batch of unseen test images.

\clearpage