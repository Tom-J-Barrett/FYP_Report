\subsection*{Objective}
There are many on line resources that are geared towards helping deep learning novices.
One of these resources is a course on Udemy titled 'Complete Guide to Tensorflow for Deep Learning with Python' \parencite{udemy}.
This course has a section on Convolutional Neural Networks which has been followed and completed.

\subsection*{Network Architecture}
The architecture for this network is very simple as it is an introductory CNN.
It consists of two convolutional layers, two max pooling layers and a fully connected layer.

\subsection*{Dataset}
This CNN is trained on the MNIST dataset.
This dataset consists of 70,000 handwritten digits \parencite{mnist}.

\subsection*{API's}
This experiment was carried out in a jupyter notebook using tensorflow.

\subsection*{Script}
\begin{lstlisting}[style=Python]
#Helper Functions

#INIT WEIGHTS
def init_weights(shape):
    init_random_dist = tf.truncated_normal(shape, stddev = 0.1)
    return tf.Variable(init_random_dist)

#INIT BIAS
def init_bias(shape):
    init_bias_vals = tf.constant(0.1, shape = shape)
    return tf.Variable(init_bias_vals)

#CONV2D
def conv2d(x, W):
    #x -> [batch, H, W, Channels]
    #W -> [filterH, filterW, ChannelsIn, ChannelsOut]
    return tf.nn.conv2d(x, W, strides = [1,1,1,1], padding = 'SAME')

#POOLING
def max_pool_2by2(x):
    #x -> [batch, H, W, Channels]
    return tf.nn.max_pool(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')

    #NORMAL (FULLY CONNECTED)
def normal_full_layer(input_layer, size):
    input_size = int(input_layer.get_shape()[1])
    W = init_weights([input_size, size])
    b = init_bias([size])
return tf.matmul(input_layer, W) + b
\end{lstlisting}

\begin{lstlisting}[style=Python]
#32 features for every 5 x 5 patch with 1(grayscale)
convo_1 = convolutional_layer(x_image, shape = [5,5,1,32])
convo_1_pooling = max_pool_2by2(convo_1)

convo_2 = convolutional_layer(convo_1_pooling, shape = [5,5,32,64])
convo_2_pooling = max_pool_2by2(convo_2)

convo_2_flat = tf.reshape(convo_2_pooling, [-1,7*7*64])
full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat, 1024))
\end{lstlisting}

\begin{lstlisting}[style=Python]
with tf.Session() as sess:
    sess.run(init)
    
    for i in range(steps):
        batch_x, batch_y = mnist.train.next_batch(32)
        batch_test = mnist.test.next_batch(32)
        sess.run(train, feed_dict = {x:batch_x, y_true:batch_y, hold_prob:0.5})
        if i%500 == 0:
            print("ON STEP: {}".format(i))
            print("ACCURACY: ")
            match = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))
            acc = tf.reduce_mean(tf.cast(match, tf.float32))
            print(sess.run(acc, feed_dict = {x:batch_test, y_true:mnist.test.labels, hold_prob:1.0}))
            print('\n')
            
    sess.run(acc, feed_dict = {x:mnist.test.images} )

#Final Accuracy 0.973
\end{lstlisting}

\subsection*{Results}
The Final Accuracy for this experiment was of 97.3\%.