\tocless\subsection{Objective}
Similar to the previous experiment, this experiment is a Udemy course exercise \parencite{udemy}. In contrast, this does not use a dataset built into TensorFlow so therefore, there is extra configuration to be done on the dataset.

\begin{table}[h]
\centering
\caption{Udemy Tutorial 2}
\label{my-label}
\begin{tabular}{|l|p{8cm}|}
\hline
\textbf{Network Architecture} & Same as \ref{udemy1}            \\ \hline
\textbf{Dataset}              & CIFAR-10 dataset \\ \hline
\textbf{APIs and Libraries}   & TensorFlow                                                         \\ \hline
\end{tabular}
\end{table}

\tocless\subsection{Script}
Key features of the script used to create the CNN for this tutorial are outlined in Figures \ref{lst:unpickle}, \ref{lst:setUpImages} and \ref{lst:trainModel2}.
Figure \ref{lst:unpickle} unpickles the dataset from the files that it is stored in.
A pickled file is a Python object that has been serialised.
Once the data has been unpickled, the data must be set up to be the correct size and shape and it must also be accessible by batches.
This is defined in Figure \ref{lst:setUpImages}.
Finally, Figure \ref{lst:trainModel2} documents the code required to train the model.

\begin{figure}[h]
\caption{Unpickle Images \parencite{udemy}}
\label{lst:unpickle}
\begin{lstlisting}[style=Python]
def unpickle(file):
    import pickle
    with open(file, 'rb') as fo:
        cifar_dict = pickle.load(fo, encoding='bytes')
    return cifar_dict

dirs = ['batches.meta','data_batch_1',
'data_batch_2','data_batch_3','data_batch_4'
,'data_batch_5','test_batch']
all_data = [0,1,2,3,4,5,6]

for i,direc in zip(all_data,dirs):
    all_data[i] = unpickle(CIFAR_DIR+direc)

batch_meta = all_data[0]
data_batch1 = all_data[1]
data_batch2 = all_data[2]
data_batch3 = all_data[3]
data_batch4 = all_data[4]
data_batch5 = all_data[5]
test_batch = all_data[6]
\end{lstlisting}
\end{figure}

\begin{figure}[h]
\caption{Set Up Images for Training \parencite{udemy}}
\label{lst:setUpImages}
\begin{lstlisting}[style=Python]
def set_up_images(self):
    # Vertically stacks the training images
    self.training_images = np.vstack([d[b"data"] for d in self.all_train_batches])
    train_len = len(self.training_images)
    
    # Reshapes and normalizes training images
    self.training_images = self.training_images.reshape(train_len,3,32,32).transpose(0,2,3,1)/255
    # One hot Encodes the training labels (e.g. [0,0,0,1,0,0,0,0,0,0])
    self.training_labels = one_hot_encode(np.hstack([d[b"labels"] for d in self.all_train_batches]), 10)

    # Vertically stacks the test images
    self.test_images = np.vstack([d[b"data"] for d in self.test_batch])
    test_len = len(self.test_images)
    
    # Reshapes and normalizes test images
    self.test_images = self.test_images.reshape(test_len,3,32,32).transpose(0,2,3,1)/255
    self.test_labels = one_hot_encode(np.hstack([d[b"labels"] for d in self.test_batch]), 10)
    
#get the next batch
def next_batch(self, batch_size):
    x = self.training_images[self.i:self.i+batch_size].reshape(100,32,32,3)
    y = self.training_labels[self.i:self.i+batch_size]
    self.i = (self.i + batch_size) % len(self.training_images)
    return x, y
\end{lstlisting}
\end{figure}

\begin{figure}[h]
\caption{Train the Model \parencite{udemy}}
\label{lst:trainModel2}
\begin{lstlisting}[style=Python]
#loss function
cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_true, logits = y_pred))
#activation function
optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)
train = optimizer.minimize(cross_entropy)
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    #for each step, get the next batch and run through the model
    for i in range(10000):
        batch = ch.next_batch(100)
        sess.run(train, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})
        
        # PRINT OUT A MESSAGE EVERY 100 STEPS
        if i%1000 == 0:
            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))
            acc = tf.reduce_mean(tf.cast(matches,tf.float32))
            testSet = ch.next_test_batch(100)
            print('Accuracy: ')
            print(sess.run(acc,feed_dict={x:testSet[0] ,y_true:testSet[1],hold_prob:1.0}))
\end{lstlisting}
\end{figure}

\tocless\subsection{Results}
A final Top-1 accuracy of 71\% was reached.

\tocless\subsection{Analysis}
Two tutorials have been carried out training models, using similar architectures, on two different datasets.
The first model was trained on the MNIST dataset and resulted in a Top-1 accuracy of 97.3\% while the second model was trained on the CIFAR-10 dataset with a Top-1 accuracy of 71\%.
Both datasets had 10 classes associated.
There are three main reasons why the Top-1 accuracy of these two models has a difference of 26.3\% which are:
\begin{enumerate}
    \item{Dataset size: In MNIST, there are 60,000 images used for training as opposed to 50,000 in CIFAR-10. The difference here would be minuscule, but it would have some impact.}
    \item{Colour channels: The images in MNIST are in black and white while the CIFAR-10 images have three colour channels associated.}
    \item{Object complexity: The objects in the the MNIST dataset are simpler shapes with a plain background. The CIFAR-10 images are more complex such as a horse and the background is noisy.}
\end{enumerate}

\clearpage
