There are various different metrics that can be used for evaluating the output
of a classifier or segmentation algorithm, many of which we have seen in previous
sections. These metrics will be examined below.

\subsection*{Research into Diagnosing Errors in Object Detectors}
There has been some research into the question of how to evaluate object
detectors, one of which be discussed in detail \textcite{diagnosingErrors}.
This paper in question "analyzes the influences of object characteristics on
detection performance and the frequency and impact of different types of false
positives" \textcite{diagnosingErrors}. They found that there were many effects
that had influence on detectors as follows:
\begin{itemize}
    \item{occlusion}
    \item{size}
    \item{aspect ratio}
    \item{visibility of parts}
    \item{viewpoint}
    \item{localization error}
    \item{confusion with semantically similar objects}
    \item{confusion with other labeled objects}
    \item{confusion with background}
\end{itemize}

The research team goes on to analyse false positives in object detectors.
Localization errors were a large factor. This is were bounding boxes overlap to
other objects in the image. Confusion with similar objects had a large influence
on false positives also by which, for example, a dog detector had a high score
for a cat \textcite{diagnosingErrors}. Confusion with dissimilar objects and
confusion with background are the categories of the rest of the false positives
they measured.

In conclusion the team would that "Most false positives are due to misaligned
detection windows or confusion with similar objects"
\textcite{diagnosingErrors}. They had some recommendations towards improves
detectors as follows:
\begin{itemize}
	\item{Smaller objects are less likely to be detected}
	\item{Localization could be improved}
	\item{Reduce confusion with similar categories}
	\item{Robustness to object variation}
	\item{More detailed analysis}
\end{itemize}

\subsection*{Detection Average Precision}
The average detection accuracy of a system. See \ref{fig:dap}.

\subsection*{Mean Average Precision}
The mean accuracy of a system across all results. See \ref{fig:MAP}.
		
\subsection*{Distribution of top-ranked false positives}
This metric is used to tell where most errors occur when false positives are
evident. These errors are broken down into four categories of localization,
similar objects, background confusion and others. See \ref{fig:DFP}.

\subsection*{Segmentation Mean Accuracy}
This is the mean accuracy of segmentation. See \ref{fig:SMP}.

\subsection*{Per-category segmentation accuracy}
This metric measures the accuracy of segmentation at a category level. See \ref{fig:PCATSA}.	

\subsection*{Per-class segmentation accuracy}
The accuracy of segmentation at a class level is analysed. See \ref{fig:PCLASSA}.

\subsection*{Top-1 and Top-5 Accuracy}
When a classifier is given an image it normally responds with a list of
predictions along with a decimal representation of the likelihood that it is of
that class. The Top-1 accuracy is the top valued prediction and Top-5 accuracy
is the top 5 predictions.
		
