\subsubsection*{Gradient Descent}
Gradient Descent is an algorithm used to find the optimal weights to produce the
smallest prediction error. It is used to overcome problems of non linearly
separable classes. Gradient descent search selects a random weight value and
then modifies it gradually to minimize the error. "At each step, the weight
vector is altered in the direction that produces the steepest descent along the
surface" \textcite{MLANN}. This step is iterated until the lowest value is met.

There is an error function used for the perecptron which finds the lowest error for that neuron, but it can't be used here because, since we have many neurons, there could be an error in multiple neurons.
Gradient Descent is mathematically based on the derivitive of a function.
The gradient of a function can be calculate by differentiating it.
As the weights are what is being controlled, "they are what we differentiate in respect to" \textcite{MLAlgorithm}.
The negative gradient of this function is followed to find the lowest possible point, hence the name gradient descent \textcite{MLAlgorithm}.

One problem with Gradient Descent is that if we look at \ref{fig:GD}, we may
never get to the optimal point, point B. This is because we will find point A
without too many problems but when the weights change we will get too high a
slope of error and therefore will never reach point B.

Another variation of Gradient Descent is Stochastic Gradient Descent (SGD). SGD
is different because it updates "weights incrementally, following the
calculation of the error of each individual example" \textcite{MLANN}. 

\begin{figure}
      \includegraphics{GradientDescent}
      \caption{Gradient Descent}
      \label{fig:GD}
 \end{figure}

\subsubsection*{Backpropagation}
"The Backpropagation algorithm learns the weights of a multilayer network,
given a network with a fixed set of units and interconnections" \textcite{MLANN}.
Backpropgation attempts to minimise the mean squared error between the target
output and the output of a network.

Backpropagation works by starting at the output layer of the network and going
back through previous hidden layers, updating weights as it goes ie. it propogates back through the network, updating the weights to try and reduce the error.
