\tocless\subsection{Implementation}
Figures \ref{lst:backendCode} and \ref{lst:labelBackend} document the code used to implement the backend service.

\begin{figure}[h]
\caption{Backend service code}
\label{lst:backendCode}
\begin{lstlisting}[style=Python]
import base64
import label_image
import time

app = Flask(_name_)
#add a route for the flask app and what HTTP methods are allowed
@app.route('/classifyImage/', methods=['POST'])
def classify():
    #decode image and save it
    imgdata = base64.b64decode(request.form.get('image'))
    filename = 'images/' + str(time.strftime("%Y%m%d-%H%M%S")) + '.jpg'
    with open(filename, 'wb') as f:
        f.write(imgdata)

    #classify the image
    results = label_image.runModel(filename)
    predictions = results[0]

    #retrun a String of the results
    result_String = ""
    for i in predictions :
        result_String += str(i) + ","
    return result_String

#runs the app on port 5000
if _name_ == '_main_':
    app.run(host='0.0.0.0', threaded=True)
\end{lstlisting}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics{secGroups}
    \caption{AWS Security Group}
    \label{fig:awsSecGroup}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics{aws}
    \caption{AWS Network Diagram}
    \label{fig:aws}
\end{figure}

\begin{figure}[h]
\caption{Method called from 'label\_image.py' - adapted from \parencite{retrainInception}}
\label{lst:labelBackend}
\begin{lstlisting}[style=Python]
def runModel(file_name):
  #load the graph using load_graph() provided be Google
  graph = load_graph(model_file)
  #read in tensor
  t = read_tensor_from_image_file(file_name,
                                  input_height=input_height,
                                  input_width=input_width,
                                  input_mean=input_mean,
                                  input_std=input_std)

  input_name = "import/" + input_layer
  output_name = "import/" + output_layer
  input_operation = graph.get_operation_by_name(input_name)
  output_operation = graph.get_operation_by_name(output_name)

  #run the tensor through the graph
  with tf.Session(graph=graph) as sess:
    results = sess.run(output_operation.outputs[0],
                      {input_operation.outputs[0]: t})
  results = np.squeeze(results)

  #get the top 5 results
  top_k = results.argsort()[-5:][::-1]
  labels = load_labels(label_file)

  setIndex = False

  #return results
  top5_results = [None] * 5
  index = 0
  for i in top_k:
    top5_results[index] = labels[i]
    index += 1

  final_results = [top5_results, results]
  return final_results
\end{lstlisting}
\end{figure}

\tocless\subsection{Deployment}
In order to deploy the Flask application to an AWS EC2 instance the following steps had to be followed:

\begin{itemize}
    \item{Upload the output\_graph.pb file (TensorFlow model), the output\_labels.txt file and the source code to the server using SFTP.}
    \item{SSH into the server.}
    \item{Create a folder called 'images' in the home directory to store uploads images.}
    \item{Place model and label files into folders called 'models' and 'labels' respectively.}
    \item{Ensure TensorFlow, Python and Flask are installed on the server.}
    \item{Ensure no processes are running on port 5000 using the command 'lsof -i :5000'.}
    \item{Use the command 'screen'.}
    \item{Run the server.py code using the command 'python server.py'.}
    \item{Use the commands CTRL+A and CRTL+D to exit screen.}
\end{itemize}

\tocless\subsubsection{AWS Architecture}
The application interfaces with an Amazon Web Services (AWS) server called an EC2 instance.
To keep the application secure AWS was used to provide a secure architecture.
A Virtual Private Cloud (VPC) was created to host the instances.
In a production environment a load balancer could be used to evenly distribute traffic across a fleet of backend instances but for the purposes of this prototype application, a single instance as in Figure \ref{fig:aws} could be used.
In Figure \ref{fig:aws} the instance is located inside a subnet which is in turn located in a VPC.
The purpose of a VPC is to have a virtual network dedicated to a users individual account. 

Security groups were utilised to restrict the type of traffic that would be allowed to reach the instance, so that the instance would only receive HTTP requests at port 5000 and receive SSH connections at port 22 as per Figure \ref{fig:awsSecGroup}.

% \begin{itemize}
%     \item{Upload the output_graph.pb file (Tensorflow model), the output_labels.txt file and the source code to the server using SFTP}
%     \item{SSH into the server}
%     \item{Create a folder called 'images' in the home directory to store uploads images}
%     \item{Place model and label files into folders called 'models' and 'labels' respectively}
%     \item{Ensure Tensorflow, NOHUP (Ignores the hangup signal in Linux) Python and Flask are installed on the server}
%     \item{Ensure no processes are running on port 5000 using the command 'lsof -i :5000'}
%     \item{Run the server.py code using the command 'nohup python server.py &'}
% \end{itemize}

\clearpage